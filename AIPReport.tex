%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Imperial College
\documentclass[a4paper,11pt,twoside]{article}
\usepackage[left=2.5cm,right=2cm,top=2cm,bottom=2cm]{geometry}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Paragraph
\usepackage[parfill]{parskip}

% Images
\usepackage{graphicx}

% URLs
\usepackage{hyperref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

\begin{titlepage}
\begin{center}

Astronomical Image Processing: Galaxy Count from a KPNO Deep Optical 
Image \\[8cm]
Dakshina Scott \\[4cm]
Somethingth November 2012 \\

\end{center}
\end{titlepage}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section*{Abstract}
% Past tense
We are presented with a deep optical image taken using a Sloan r-band
filter on the 4m telescope using the CCD mosaic camera at Kitt Peak 
National Observatory.

A script is written in MatLab to detect galaxies within the image,
and catalogue their brightnesses. A plot of galaxy counts against
magnitude is found to differ significantly from similar surveys and
from the equation characterising the relation between counts and
magnitude for an Euclidean universe i.e. . The results fit a
quadratic curve much better than the predicted straight line, with
a steepest gradient of 0.4 plus/minus something. This is thought to
be mostly likely due to incompleteness of the image, rather 
than evidence of strong galaxy evolution or structural information 
about the universe.
% Remove comma
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\tableofcontents
\newpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Background & Introduction}

Galaxy surveys allow us to probe the history and structure of the
universe. Number counts, spectral analysis and photon counts from 
such surveys have helped to transform our understanding of the 
universe – when in 1917 Einstein's theory of general relativity %new line
suggested that the universe must be either expanding or contracting, % the idea was brushed?
he brushed the idea aside. It was only after Silpher and Hubble 
produced data from galaxy surveys relating redshift and distance 
that the evidence in support of an expanding universe became 
impossible to ignore[An Introduction to The Science of Cosmology, D J Raine, E G Thomas, p60]. %ref

Here we look at a deep optical image taken using a Sloan r-band 
filter (central wavelength 620nm) using the CCD mosaic camera at %a CCD
Kitt Peak National Observatory. Only the r-band photon counts, and 
no spectroscopic data, are available. The types of the galaxies are 
mostly indistinguishable. The aim is to find the number counts of 
galaxies at different magnitudes, and compare our results to the 
prediction for an Euclidean universe of galaxies free from evolution 
effects described by equation (number).
We also compare our counts to similar surveys in order to evaluate 
our cataloguing procedure.

By assuming a Euclidean universe of uniformly distributed galaxies, 
with a given distribution of luminosities, a simple relationship 
between magnitude and number of galaxies brighter than that 
magnitude can be derived. For a given luminosity distribution;
 
and so,	eq.(1)

\[k = \frac{x'y''-y'x''}{(x'^2+y'^2)â€‹^{3/â€‹2}â€‹}\]

and for a given density in Euclidean space;

.	eq.(2)

Substituting equation (1) into equation (2) we get;

	eq.(3)

where N(f) is the number of sources with flux greater than f.
Using



which is the standard equation relating the difference in magnitude 
between two objects and the flux received from them, it can be 
seen that;


	
rearranging for f;

		eq.(4)

and substituting eq.(4) into eq.(3);

or

where k is a constant of proportionality. Finally, taking log of 
both sides;



where N(m) is the number of sources brighter than magnitude m, and .

Based on this result, the difference in the number of galaxies 
between each magnitude should be given by


where m is the lower magnitude.

The Friedmann equation tells us that the curvature of the %equations
universe is dependent on it's density [Cosmological Physics, J A Peacock, p73] – %\ref - insert eq?
it is not necessarily Euclidean so there will be a distance 
beyond which the structure of the universe affects the results.

One of the key results looked at in galaxy surveys (eg Koo 1981) 
is how the data deviates from the counts expected by a 
no-evolution model of galaxies, thus trying to determine how 
the galaxies have actually evolved – the further away the 
galaxy, the further in the past we are looking – so say 
for example galaxies were brighter/dimmer or more/less 
numerous, then distant galaxies may give us results inconsistent 
with equation (number).

In 1961 Sandage (reference) found that galaxy counts are unlikely to reveal information about the structure of the universe because the differences between flat, open and closed models are too small compared to known variations in galaxy distribution. However, advances have been made in astronomical image acquisition. CCDs allow much greater quantum efficiency than photographic plates – as many as 90\% of the photons falling on a CCD will be detected, around 9 times the efficiency of the best photographic plates. Eisenstein et al (2001) found that the Sloan Sky Digital Survey, when complete, would be well suited to studies of large-scale structure. 
Thus it is possible that comparing number counts against magnitude with the prediction of equation (number) will show evidence of galaxy evolution or even the large-scale structure of the universe.

\section{Method}

Although the image had been processed to adjust for certain effects before it was received, there were still some unwanted effects remaining. Image processing and analysis is done in Matlab, with the image processing toolbox installed.

\subsection{Noise Removal}

The image consists of a number of sub-exposures, combined to create one image. This smooths out random fluctuating noise , as inconsistent sources will be faded in the final image.
However, it can be seen that these sub-exposures don't overlap exactly – as a result there are blank areas in the corners of the image, and around the edges a sharp increase in noise is visible (see figure). 
Figure – red lines are to highlight sharp increases in noise where sub-exposures don't overlap. The top right corner is blank.
The image was cropped before analysis to remove the noisiest areas.

\subsection{Saturated Pixels}

The image shows a bright central start which has 'bloomed' vertically across the image. There are also smaller stars showing the same effect to a lesser extent. This blooming occurs where pixels in the CCD are saturated and overflow to nearby pixels, something which tends to happen mostly in the vertical and horizontal directions, as can be seen in figure~\ref{fig:plot}.

\begin{figure}[htb]
  \centering
  \includegraphics[width=0.9\textwidth]{plot.png}
  \caption{Matlab plot of stuff.}
  \label{fig:plot}
\end{figure}

The maximum number of electrons each pixel in the CCD can hold is 65,535. For values approaching this (~50,000 according to labscript – header file suggests 36,000 is maximum good data value)\cite{yasuda} [1] the relationship between number of photons received and the number of electrons stored moves further and further from one-to-one (was it ever?).
By using multiple shorter exposures any given CCD pixel receives fewer photons, and the number of sources bright enough to cause overflowing is reduced.
However this effect is still visible in some sources (highlighted in figure) . These sources and their bloom are blacked out before analysis.

A filter is also applied, so that any remaining sources with a maximum pixel value exceeding 50,000 counts are excluded from the survey.

On this fully prepared image, two different algorithms for source detection and photometry are applied, each with different limitations. One method uses a fixed aperture, while the other uses the shape and size of each individual source to analyse the correct areas of the image. Both methods are dependent upon finding a suitable value for the source/background threshold.

\subsection{Finding an Upper Limit for Background Brightness}

In order to analyse each source in the image, some criterion must be used to determine how high a photon count must be in order for it to be considered to be coming from a source, rather than a background fluctuation.
The image is composed mostly of background, with a scattering of sources of varying brightness. Thus finding the mode pixel intensity value of the image would be one way to estimate the background value – doing so produces a value of 3421 counts. However, as the background isn't a constant value, it's clear that much of the background will have values higher than this, and some upper limit of counts must be chosen as the threshold.
Inspecting the distribution of photon counts within the image shows that the frequency of counts per pixel in the region of this modal value is many orders of magnitude greater than that for other values found in the image, and the distribution within this area is approximately Gaussian (see figure). Thus by finding the mean value within just this bell-shaped peak, and the standard deviation, a threshold value is chosen as three standard deviations above the mean (at a value of 3461 counts).

Figure – Histogram showing a Gaussian-like distribution of pixel counts in the region of the global background.

\section{Fixed Aperture}

\subsection{Finding Sources}

Initially sources are identified and analysed using a fixed aperture size. The maximum pixel value in the image is found, and the surrounding area analysed. This area is then blacked out so that a search for the new maximum will find the second-brightest source from the original image, which is then analysed, and so on. This continues looping around until the maximum remaining value equals the threshold value.

A number of different aperture sizes are tried in an attempt to find a compromise between a very large aperture and a very small aperture – any source larger than the chosen aperture size will end up with a lower count attributed to it than it should have, reduced even further when the 'local background' (which in this case will at least partially be obscured by the source) is subtracted. Also, the smaller the aperture the more opportunity there is for each source to be counted as multiple sources (although measures are taken within the code to reduce this – see figure). On the other hand, sources which are smaller than the aperture will have a reduced mean intensity value, although the total number of counts should be about right once the local background is subtracted. 
Figure – black circles represent areas that have been treated as sources by the detection program. (a) shows those detected before code was added telling the program to ignore aperture areas which already contained a zero value. (b) shows areas of source detected after this section of code was added. As can be seen, there are some sources which aren't contained by the aperture. 

As the aperture size is increased, more than one source is likely to be contained within a single aperture. Thus an appropriate size would be a compromise. The mean value of the source areas is 51 pixels, suggesting an aperture radius of 4 pixels may be a reasonable compromise. However, as some of the sources are much larger than this, the number counts will be skewed – if lots of apertures fit inside one large source, this will give rise to a jump in the frequency distribution in the range of brightnesses contained in the source.

For each source, an aperture of recommended (labscript) radius 6 pixels is used as a starting point (~3 arc seconds – do calculation?) [reference (arc seconds per pixel)], centred on the maximum value. A larger-radius annular aperture is then placed around this to calculate the background level local to each source, which is then subtracted from the source aperture value. 
Different radii are compared, including a radius of 4 pixels.

\subsection{Variable Aperture Method}

Two methods were developed using a variable aperture size to analyse the sources.
Of the two, the first has some significant issues – in cases when the multiple sources are within a few pixels of each other, it's possible that the same source will be analysed multiple times, or it may be skipped altogether. The second version achieves more reliable results. By making better use of inbuilt Matlab functions, the new program was much more effective at quickly detecting and analysing sources (it runs in about 3 seconds). It turns out that both methods appear to produce quite similar results, but only the latter is discussed from here on, as it most accurately represents the data available.

'Thresholding' is used to segment the image based on pixel counts – the image is converted to a binary image, in which any pixels with a count higher than the source/background threshold (as determined above) become ones, the rest zeros. By assigning distinct labels to each separate region of ones, Matlab is then able to run through them sequentially, analysing each corresponding source \& local background as it goes.
An issue with this method is that sometimes part of the source is 'disconnected' from the main section (as can be seen in the top left and bottom right sides of the binary image in figure).
Figure – a section of the image in grayscale, and the same section in binary, showing that a good approximation of the source shape and area is obtained. However, notice the small white areas near sources 1 and 2. These will be counted as separate sources, regardless of whether they are legitimate sources or disconnections from nearby sources.

These disconnections will lead to an increase in counts of faint sources.

Another issue arises with finding the local background for each source. To do so an area surrounding each source is looked at. If multiple sources are close together and their 'local background areas' overlap then the same local background, averaged over the regions local to each source, is used for all of these sources. (image to illustrate?)

Despite this, the variable aperture method is the preferred method as it always analyses all of the sources in the image (so long as the threshold allows them all to be detected), and unlike the fixed aperture it analyses the actual shape and area of each source (again limited by the accuracy of the threshold used).

\section{Results & Analysis}

First the results of the fixed aperture method are looked at, with different aperture sizes compared. Then the variable aperture method is looked at in more detail and finally the two sets of results are briefly compared.
The raw photon counts are converted to magnitudes using the equation,


where ZPinst is the instrumental zero point, as determined in the AB magnitude system (where a flux density of 3631 Jy is defined to be zero magnitudes). 
According to the fits header for the image the maximum reliable value for photon counts per CCD pixel is 36000 – for a one-pixel source this translates to a magnitude of 14. Although we have excluded single-pixel sources, this suggests that for magnitude 14 and brighter, more sources will have unreliable results, as determined by the limitations of the equipment used. This should be kept in mind when looking at the results.

\section{Fixed Aperture}

Number count against magnitude is plotted for various aperture radii. The 4-pixel radius appears to produce the straightest graph. This could imply that the 'wobbles' in the other graphs are due to the limits of a fixed aperture rather than a physical phenomenon. 



Figure? shows that an aperture radius of 4 pixels has the 'straightest' graph. As the radius diverges from this the data points become more and more 'wobbly'. In all cases, just below magnitude 14 there is a dip in the graph. 

Variable Aperture
A variable aperture method is able to much more accurately collect data on each source in the image.

Figure – the errors shown are the errors in the number counts of galaxies following a poisson distribution. The largest error in the x-axis is 0.023. This is a combination of the calibration error on the apparatus used and the error in photon counts following a poisson distribution The solid line shows a quadratic best fit curve. The dashed line shows a gradient of 0.6. The errors in magnitude aren't shown as they are too small to show up on the graph (see appendix for more details).

This graph shows much more curvature than the fixed aperture graphs. This is expected as galaxies become gradually more difficult to detect at higher magnitudes, so the gradient represents the gradual increase of incompleteness in the survey.
Excluding the sources which contain a pixel value of over 36000 (the maximum good data value in the fits header) has very little effect – only 14 sources are excluded and the resulting graph looks the same.

The gradient of the graph up to magnitude 11 is about 0.4. As can be seen by the curvature of the graph this gradually decreases, reaching 0.1 between 18 and 20, and it is finally zero (to one decimal place) between 21 and 23. 
On the other hand a quadratic least squares line of best fit, as shown in the figure, can be seen to fit the data very closely. The gradient of this curve against magnitude is shown in figure below.
Figure – shows how the gradient of log(N(m)) against magnitude (approximated as a quadratic curve) changes with magnitude. By magnitude 20, the gradient is very close to zero.

Even the maximum gradient of about 0.4 is much lower than the gradient of 0.6 predicted by equation (number). A number of factors will likely have combined to result in this low value. If the survey was complete, there would be an increase in the galaxy counts at higher magnitudes.


Yasuda et al (2001) present galaxy surveys in the northern and southern hemispheres, with very high levels of completeness, in a variety of bands including r-band. At lower magnitudes (where effects of evolution and large-scale structure aren't significant) their number counts fit equation (number) well. 
This suggests that our data may suffer from incompleteness – and that this effect worsens at increasing faintnesses.
The gently decreasing gradient fits with this suggestion, as incompleteness would be expected to  increase gradually with magnitude.
Removing the brightest sources (which exhibited a blooming effect) also may have affected the gradient – the brightest sources may have decreased the starting value on the x-axis.




\section{Conclusion}

As discussed the variable aperture is believed to be much more reliable and accurate in terms of detecting and collecting data on each source in the image.
While the program using a fixed aperture gives results that are closer to those predicted by theory and found by other surveys, it is suspected that this is in fact misleading – that perhaps the unquantified systematic errors in the fixed aperture actually caused the data to appear closer to the predictions than the true data should be.
This fits the results found for the variable aperture, as it suggests high levels of incompleteness starting even at the brighter magnitudes.
Using a larger number of shorter exposures would improve the results for the brightest sources as less would need to be blacked out before analysis. However, this would reduce completeness at the fainter magnitudes.
Alternatively using a better CCD (e.g. with a higher bit counter) would vastly improve the results – long exposure times could be used allowing fainter sources to be detected more reliably, without causing so many pixels to saturate and bloom (so the results would also be more reliable for brighter sources).
Another issue is that the detecting programs can't distinguish between a) stars and galaxies, and b) two overlapping sources. Both issues can be addressed by looking at more detailed photometry for each source – for example, Yasuda et al (2001) use a program called 'PHOTO' which looks for multiple peaks in each object, and 'deblends' them into multiple 'child' objects. This program also able to determine whether a source is likely to be a star or a galaxy based on its surface brightness profile (as described by Blanton et al, 2001).

\section{References}

\begin{thebibbliography}{30}

\bibitem{yasuda}
Yasuda, N., et al. 2001, Galaxy Number Counts From The Sloan Digital Sky Survey Commissioning Data, AJ, 122, 1104-1124

\bibiten{eisen}
Eisenstein, D. J., et al. 2001, Spectroscopic Target Selection For The Sloan Digital Sky Survey: The Luminous Red Galaxy Sample, AJ, 122, 2267-2280

\bibitem{blanton}
Blanton, M. R., et al. 2001, The Luminosity Function Of Galaxies In SDSS Commissioning Data, AJ, 121, 2358-2380

\end{thebibbliography}

\section{Appendix}

\subsection{Calculating Errors in Magnitude}

The errors in magnitude are excluded from the graphs as they are very small – and in fact, none are large enough to affect the number of galaxies in each magnitude bin. 
They are approximated by treating the photon counts for each galaxy as a poisson distribution, with the error as the square root of the counts. These errors in counts are then converted into errors in magnitude, and finally added in quadrature with the zeropoint error included in the fits header file, to give the overall errors in magnitude for each galaxy. The largest error is 0.0227. As we are working with magnitude bins of 0.5, it's meaningless to talk about errors to 4 decimal places. To two decimal places, all of the errors are equal to 0.02, the value given for the zeropoint error. 
Figure – Distribution of errors with magnitude shows that the errors are greatest for the faintest galaxies, but all of the errors are very small.

\end{document}
